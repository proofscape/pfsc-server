# --------------------------------------------------------------------------- #
#   Copyright (c) 2011-2023 Proofscape Contributors                           #
#                                                                             #
#   Licensed under the Apache License, Version 2.0 (the "License");           #
#   you may not use this file except in compliance with the License.          #
#   You may obtain a copy of the License at                                   #
#                                                                             #
#       http://www.apache.org/licenses/LICENSE-2.0                            #
#                                                                             #
#   Unless required by applicable law or agreed to in writing, software       #
#   distributed under the License is distributed on an "AS IS" BASIS,         #
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  #
#   See the License for the specific language governing permissions and       #
#   limitations under the License.                                            #
# --------------------------------------------------------------------------- #

"""
Utilities for building Proofscape repos.

Building may be thought of as analogous to compiling and linking a C++ project. Here, the
"compiling" means writing dashgraph and annotation files, while the "linking"
means updating targeting/expansion relationships in the graph database.

More specifically, building a module entails the following operations:

    - The module is parsed and an internal representation is formed.
      In particular this means that any syntactic errors will be discovered and an
      exception will be raised.

    - The `manifest.json` file for the repo to which the module belongs is updated.

    - For every deduction defined in the module, a dashgraph is built and written to disk
      in the installation's BUILD_ROOT as a .dg.json file. This also means that any semantic
      errors in deduction definitions will be caught, and exceptions raised.

    - For every annotation defined in the module, the notes page data are built and written to
      disk, again under the BUILD_ROOT, as a pair of .anno.html and .anno.json files.

    - If the repo defines an index.rst at the root level, then a nested Sphinx build
      is carried out. All rst files defined in the repo result in Proofscape modules
      that define at least a `SphinxPage` (of name `_page`, in this module). The HTML
      generated by Sphinx is saved in under BUILD_ROOT.

    - The graph database in which all targeting relations are indexed is updated.

Builds are always on whole repos, but the system tries to re-read and re-parse
only those modules that have changed since last build. Since parsing is by far
the slowest part of the process, this should save considerable time on rebuilds.
"""

import os, json, math, re
from collections import deque
from datetime import datetime
import logging
import pathlib
import traceback

from sphinx.cmd import make_mode
from sphinx.application import Sphinx
from sphinx.errors import SphinxError
from sphinx.util.console import strip_colors
from sphinx.util.docutils import patch_docutils, docutils_namespace

from pfsc.build.mii import ModuleIndexInfo
from pfsc.build.manifest import (
    build_manifest_tree_from_dict,
    build_manifest_from_dict,
    load_manifest,
    Manifest,
    ManifestTreeNode
)
from pfsc.excep import PfscExcep, PECode
from pfsc.build.lib.libpath import PathInfo
from pfsc.build.products import get_dashgraph_dir_and_filename, get_annotation_dir_and_filenames
from pfsc.build.repo import RepoInfo, checkout, get_repo_info, parse_module_filename
from pfsc.build.versions import version_string_is_valid
from pfsc.gdb import get_graph_writer, get_graph_reader, building_in_gdb
from pfsc.constants import IndexType, PFSC_EXT, RST_EXT
from pfsc.lang.modules import (
    CachePolicy, load_module, PfscDefn, PfscAssignment,
    pickle_module, unpickle_module, remove_all_pickles_for_repo
)
from pfsc.lang.annotations import Annotation
from pfsc.lang.deductions import Deduction, Node, GhostNode
from pfsc.lang.widgets import GoalWidget
from pfsc.sphinx.sphinx_proofscape.pages import (
    build_libpath_for_rst, SphinxPage,
)

import pfsc.util
import pfsc.constants


def build_repo(
        target, version=pfsc.constants.WIP_TAG,
        verbose=False, progress=None,
        make_clean=False
):
    """
    Build a Proofscape repo.

    :param target: Either the libpath (str) of the repo that is to be built,
        or a Builder instance representing that repo.
    :param version: the version to be built
    :param verbose: as for the Builder class, except may also be set to the
        integer 2 in order to add performance output.
    :param progress: as for the Builder class.
    :param make_clean: as for the Builder class.

    :return: the Builder instance that performed the build.
    """
    if not isinstance(target, Builder):
        b = Builder(
            target, version=version,
            verbose=verbose, progress=progress,
            make_clean=make_clean
        )
    else:
        b = target
    if verbose == 2:
        profile_build_write_index(b)
    else:
        b.build_write_index()
    return b


def profile_build_write_index(builder):
    import cProfile
    with cProfile.Profile() as pr:
        builder.build_write_index()
    import pstats, io
    s = io.StringIO()
    # sortby = pstats.SortKey.TIME
    sortby = pstats.SortKey.CUMULATIVE
    ps = pstats.Stats(pr, stream=s).sort_stats(sortby)

    # Use these, to focus on indexing:
    #ps.print_stats('pfsc/build/__init__.py', 12)
    #ps.print_stats('ix0')

    # Use this instead, to just see the 50 slowest things:
    ps.print_stats(50)

    r = s.getvalue()

    # printing to stdout is fine for local unit testing, and working with the OCA
    print(r)

    # When working with the MCA, long-running tasks tend to be carried out by
    # the RQ worker, so you need to uncomment these lines to see what's going
    # on in there:
    #from pfsc.methods import log_within_rq
    #log_within_rq(r)


class BuildMonitor:
    """
    Manages monitoring of the build process.

    This class is designed to be flexible, and support different usage patterns.

    In some cases, your build process may be "distributed," in the sense that
    many different bits of code are responsible for advancing the process, one
    piece at a time. In such cases, it is handy to use this class's
    `begin_phase()`, `inc_count()`, and `set_message()` methods.

    In other cases, you may have a centralized process that keeps track of its
    own progress. Then it may be easier to use this class's `set_state()`
    method directly.
    """

    def __init__(self, external):
        """
        :param external: A function to which progress information is to be passed when available.
                         Should accept four args: (op_code, cur_count, max_count, message)
        """
        self.external = external

        self.op_code = None
        # cur_count may be integer or float
        self.cur_count = 0
        # max_count must be an integer
        self.max_count = 100
        self.message = ''

    def pub(self):
        """
        Publish the current state to the external monitor.
        """
        # In some build scenarios, we don't use an external monitor, so we
        # do have to check here.
        if self.external:
            c = min(self.max_count, math.floor(self.cur_count))
            self.external(self.op_code, c, self.max_count, self.message)

    def set_message(self, message):
        """
        Set the message.
        """
        self.message = message
        self.pub()

    def begin_phase(self, max_count, message):
        """
        Start a new phase. Indicate the total number of steps it will involve,
        and set an initial message.
        """
        self.cur_count = 0
        self.max_count = max_count
        self.message = message
        self.pub()

    def inc_count(self, f=1.0):
        """
        Increase the current count. Say by how much, or default to 1.

        f: int or float
        """
        self.cur_count += f
        self.pub()

    def set_count(self, f):
        """
        Set the current count directly, instead of by differential.

        f: int or float
        """
        self.cur_count = f
        self.pub()

    def set_state(self, cur_count, max_count, message):
        """
        Set the complete state.
        """
        self.cur_count = cur_count
        self.max_count = max_count
        self.message = message
        self.pub()

    def declare_complete(self):
        """
        Call this method when the entire build process is complete.
        """
        self.message = 'Done'
        self.cur_count = self.max_count
        self.pub()


class SphinxBuildLogStream:
    """
    Allows us to monitor the build progress of the Sphinx build (if any)
    contained within a repo build.

    Construct a logging.StreamHandler with this as stream, and then add the
    handler to the logger named 'sphinx.sphinx.util'.

    Note that Sphinx prepends the namespace prefix 'sphinx.' onto the usual
    `__name__`. This therefore is the logger defined in sphinx/util/__init__.py.

    See also sphinx/util/logging.py for the `SphinxLoggerAdapter` class that wraps
    Python's built-in logger.

    See also `sphinx.util.status_iterator()` for the code that generates the
    log messages that do contain percentages.

    """

    def __init__(self, monitor):
        """
        monitor: an instance of the BuildMonitor class
        """
        self.monitor = monitor
        self.percentage_pattern = re.compile(r'\[\s*(\d+)%\]')
        # We use a "real" counter for steps where Sphinx gives a percentage.
        self.last_real_count = 0
        # We use a "fake" counter for sequences of steps that have no percentage.
        self.last_fake_count = 0
        # These are depicted as being out of a fake denominator, which is
        # always at least a certain way away.
        # Example: with basic denom of 10, and min remainder of 3, the fractions
        # will go: 1/10, 2/10, ..., 7/10, 8/11, 9/12, 10/13, ...
        self.min_fake_rem = 3
        self.basic_fake_denom = 10

    def write(self, record):
        message = strip_colors(record.strip())
        if message == 'done':
            # Sphinx ends each of its steps with a 'done' message.
            # We're not going to display these.
            pass
        else:
            parts = self.percentage_pattern.split(message)
            if len(parts) == 3:
                # It's a percentage message
                try:
                    count = int(parts[1])
                except ValueError:
                    count = self.last_real_count
                denom = 100
                self.last_real_count = count
                message = parts[0] + parts[2].strip()
                # Reset fake counter, for next fake phase (if any)
                self.last_fake_count = 0
            else:
                # It's a step that has no percentage
                count = self.last_fake_count + 1
                self.last_fake_count = count
                denom = max(self.basic_fake_denom, count + self.min_fake_rem)
            message = 'Sphinx: ' + message
            self.monitor.set_state(count, denom, message)

    def flush(self):
        """
        Just implementing the `stream` interface so that an instance of this
        class can be passed when constructing a `logging.StreamHandler()`.
        """
        pass


class OriginInjectionVisitor:

    def __init__(self, lp2origin):
        """
        :param lp2origin: dict mapping libpaths to origins
        """
        self.lp2origin = lp2origin
        self.graph_reader = get_graph_reader()

    @staticmethod
    def takes_origin(item):
        return isinstance(item, (Deduction, Node, GoalWidget))

    def __call__(self, item):
        if self.takes_origin(item):
            lp = item.getLibpath()
            if lp in self.lp2origin:
                item.setOrigin(self.lp2origin[lp])

        if isinstance(item, GhostNode):
            real_obj = item.realObj()
            if not real_obj.getOrigin():
                realpath = real_obj.getLibpath()
                if realpath in self.lp2origin:
                    real_obj.setOrigin(self.lp2origin[realpath])
                else:
                    # This case arises if e.g. the repo X being built cites a theorem
                    # in repo Y. Since repo Y is not being built at this time, we
                    # have no origin data for its nodes, and need to consult the GDB.
                    #
                    # What we're trying to do here is ensure that the `realOrigin`
                    # property in the dashgraph for a GhostNode (see the
                    # `pfsc.lang.deductions.GhostNode.buildDashgraph()` method) has
                    # a value.
                    #
                    # This only matters if we want goal boxes to be able to appear
                    # on ghost nodes for cited theorems and lemmas. For now at least,
                    # we think we do want this. E.g. it lets the user know, without
                    # opening a theorem, whether they have already studied it and
                    # put a checkmark on it.
                    #
                    # TODO: Could we make this more efficient by noting all ghost
                    #  nodes to external repos during the build process, and then
                    #  making a single query to the GDB for all the origin info at once?
                    vers = real_obj.getVersion()
                    label = real_obj.get_index_type()
                    origins = self.graph_reader.get_origins({label: [realpath]}, vers)
                    if realpath in origins:
                        real_obj.setOrigin(origins[realpath])


class Builder:
    """
    Builds Proofscape modules.
    """

    def __init__(
            self, repopath, version=pfsc.constants.WIP_TAG,
            verbose=False, progress=None,
            make_clean=False
    ):
        """
        :param repopath: libpath of the repo to be built.
        :param version: the version we are building. Must be either "WIP", meaning we want
          to build our work-in-progress, or else a valid release tag `vM.m.p`.
        :param verbose: control printing
        :param progress: a function to which to pass progress updates
        :param make_clean: set True to erase any and all existing pickle files
            for the modules in this repo@version, as well as to `make clean`
            with Sphinx (if a part of the build), all before beginning this build
        """
        self.repopath = repopath
        self.version = version
        self.verbose = verbose
        self.make_clean = make_clean
        self.monitor = BuildMonitor(progress)
        self.graph_writer = get_graph_writer()
        self.build_in_gdb = building_in_gdb()

        if not version_string_is_valid(version, allow_WIP=True):
            raise PfscExcep(f'Invalid version string: {version}', PECode.MALFORMED_VERSION_TAG)

        # We set a couple of arbitrary counts for progress monitoring.
        self.prog_count_per_module = 100
        self.prog_count_for_parsing = 10

        self.timestamp = None

        self.repo_info = get_repo_info(self.repopath)
        self.commit_hash = self.repo_info.get_current_commit_hash()

        self.module_cache = {}

        self.mii = ModuleIndexInfo(
            self.monitor,
            self.repopath,
            self.version,
            self.commit_hash,
            recursive=True
        )

        # lookups by libpath
        self.deductions = {}
        self.annotations = {}
        self.sphinx_pages = {}
        self.modules = {}

        self.repo_node = ManifestTreeNode(self.repopath, type="MODULE", name=self.repopath)
        self.manifest = Manifest(self.repo_node)

        # A "reading job" is a pair (modpath, ManifestTreeNode), representing
        # a module that is to be loaded.
        self.reading_jobs = None
        # A "scan job" is a pair (PfscModule, ManifestTreeNode), representing
        # a module that we have loaded from disk, but have not yet analyzed its
        # contents.
        self.scan_jobs = deque()

        # A place to store a copy of the dependencies declared by the repo being built:
        self.repo_dependencies = {}

        # Set up items to be skipped.
        # For now we automatically skip any directory or file beginning with a dot '.'
        # TODO:
        # In the future, we may want to allow repos to define a `.pfscignore` file where they can specify more skip-paths.
        # Format:
        #   path-within-repo points to dict optionally defining 'dirs' and 'files' keys, under each of which is
        #   a list of dirnames or filenames to be skipped under this path.
        self.skip_items = {
        }

    def get_repo_deps(self):
        return self.repo_dependencies

    def is_release_build(self):
        return self.version != pfsc.constants.WIP_TAG

    def has_sphinx_doc(self):
        p = pathlib.Path(self.repo_info.abs_fs_path_to_dir) / 'index.rst'
        return p.exists()

    def raise_missing_change_log_excep(self):
        msg = f'Repo `{self.repopath}` failed to declare a change log for release `{self.version}`'
        msg += ', which is a major version increment.'
        raise PfscExcep(msg, PECode.MISSING_REPO_CHANGE_LOG)

    def build_write_index(self):
        """
        Conduct the entire process of three phrases: (1) Build, (2) Write, and (3) Index

        :return: nothing
        """
        self.build()
        self.update_index()
        self.write_all()
        self.monitor.declare_complete()

    def build(self, force_reread_rst_paths=None, no_sphinx_write=False):
        """
        Here is where we do the actual building operations.

        :param force_reread_rst_paths: optional list of filesystem paths to rst files
            which you want Sphinx to re-read (even if they have not been modified
            since last build)
        :param no_sphinx_write: set True if you want Sphinx to read rst files,
            form, pickle, and resolve doctrees, but not write any output files.
        :return: nothing
        """
        if self.make_clean:
            remove_all_pickles_for_repo(self.repo_info.libpath, version=self.version)

        self.monitor.set_message('Starting...')
        with checkout(self.repo_info, self.version):
            self.reading_jobs = self.walk(self.repo_info.abs_fs_path_to_dir)
            # Copy the source files to the build dir.
            # If building at a numbered version, these are needed both by our
            # own build process (so `load_module()` can find them), and so that
            # non-owning users can browse the source.
            # They are also needed for imports during other builds.
            for modpath, _ in self.reading_jobs:
                pi = PathInfo(modpath)
                path = pi.get_build_dir_src_code_path(version=self.version)
                if not path.parent.exists():
                    path.parent.mkdir(parents=True)
                # Read @WIP, since we want the currently checked-out version.
                src = pi.read_module(version=pfsc.constants.WIP_TAG)
                path.write_text(src)

            self.check_root_declarations()
            self.mii.compute_mm_closure(self.graph_writer.reader)

            # For the sake of any Sphinx build we might do, we stay inside the
            # checkout() context. Sphinx needs the version being built to still
            # be checked out. *Could* consider copying the whole repo dir to a
            # tmp dir, and then setting `sourcedir` in `self.build_sphinx_doc()`
            # accordingly. For now we don't do that. (The copies we just placed
            # in the `build` dir are not structured correctly to be used in this way.)

            # If you want a Sphinx build to take place, you have to have an `index.rst` file
            # in the repo root dir.
            if self.has_sphinx_doc():
                if self.build_in_gdb:
                    msg = (
                        'rst modules are not yet supported with `build_in_gdb` option.'
                        ' Please contact your system admin, or report this as a bug.'
                    )
                    raise PfscExcep(msg, PECode.OPTION_NOT_SUPPORTED_WITH_BUILD_IN_GDB)
                # Our own `self.read_and_resolve()` will be called during our
                # handler for the Sphinx 'env-updated' event. This results in the following
                # order of events:
                #   Sphinx READ -> Proofscape READ -> Proofscape RESOLVE -> Sphinx RESOLVE
                # In particular, all modules have been read before any module tries to resolve.
                self.build_sphinx_doc(
                    force_reread_rst_paths=force_reread_rst_paths,
                    just_read_no_write=no_sphinx_write
                )
            else:
                self.read_and_resolve()

    def read_and_resolve(self):
        self.reading_phase()
        self.resolving_phase()

    def reading_phase(self):
        """
        Form modules by READING pfsc files, but do not yet RESOLVE them.

        In cases where we're going to do a Sphinx build, this function
        should be carried out after the Sphinx build has
        finished its READING phase, but before it begins its RESOLVING phase.
        """
        self.monitor.begin_phase(self.prog_count_per_module * len(self.reading_jobs), 'Reading...')

        for modpath, mod_node in self.reading_jobs:
            module = self.read_pfsc_module(modpath)
            self.scan_jobs.append((module, mod_node))
            self.modules[module.libpath] = module

    def resolving_phase(self):
        """
        RESOLVE the pfsc modules formed in the READING phase.
        """
        modules = list(self.module_cache.values())
        for module in modules:
            module.resolve(cache=self.module_cache)

        while self.scan_jobs:
            module, manifest_node = self.scan_jobs.popleft()
            self.scan_pfsc_module(module, manifest_node)

        self.mii.cut_add_validate()
        self.mii.here_elsewhere_nowhere()
        self.mii.compute_origins(self.graph_writer.reader)
        self.inject_origins()
        self.timestamp = datetime.now()
        self.manifest.set_build_info(self.repopath, self.version, self.repo_info.git_hash, self.timestamp)

    def build_sphinx_doc(self, force_all=False, filenames=None,
                         force_reread_rst_paths=None,
                         just_read_no_write=False):
        """
        :param force_all: write all files, instead of just for new or changed source
            files. Same as `-a` siwtch to commandline Sphinx.
        :param filenames: try to build only these output files. Same as passing filenames
            to commandline Sphinx.
        :param force_reread_rst_paths: optional list of filesystem paths to rst files
            which you want Sphinx to re-read (even if they have not been modified
            since last build)
        :param just_read_no_write: set True if you want Sphinx to read rst files,
            form, pickle, and resolve doctrees, but not write any output files.

        See:
            https://www.sphinx-doc.org/en/master/man/sphinx-build.html#cmdoption-sphinx-build-a
            https://www.sphinx-doc.org/en/master/man/sphinx-build.html#synopsis
        """
        sourcedir = self.repo_info.abs_fs_path_to_dir
        confdir = sourcedir
        outputdir = self.repo_info.get_sphinx_build_dir(version=self.version)
        doctreedir = os.path.join(outputdir, '.doctrees')

        p = pathlib.Path(outputdir)
        p.mkdir(parents=True, exist_ok=True)

        if self.make_clean:
            args = ['clean', sourcedir, outputdir]
            make_mode.run_make_mode(args)

        # Conf overrides
        required_extensions = [
            'sphinx_math_dollar',
            # We use the separate extension for the lexers:
            'sphinx_proofscape',
            # We use the local extension for everything else:
            'pfsc.sphinx.sphinx_proofscape',
        ]
        confoverrides = {
            'pfsc_repopath': self.repo_info.libpath,
            'pfsc_repovers': self.version,
            'html_theme': 'furo',
            # Note: this does not extend, but overwrites completely any 'extensions' list
            # the user may have specified in their conf.py. This is fine for now since we
            # are not yet supporting any way of adding extensions to the Proofscape build
            # system.
            'extensions': required_extensions,
        }

        stream = SphinxBuildLogStream(self.monitor)
        handler = logging.StreamHandler(stream)
        logger = logging.getLogger('sphinx.sphinx.util')
        logger.addHandler(handler)

        def add_rereads(app, env, docnames):
            # Sphinx is unaware of our pickle files, so it's up to us to ensure
            # they stay up to date.
            # Consider existing docs that Sphinx is not already planning to re-read:
            for docname in env.found_docs - set(docnames):
                modpath = build_libpath_for_rst(app.config, docname, within_page=False)
                u = unpickle_module(modpath, self.version)
                if not u:
                    docnames.append(docname)
                else:
                    t_r = u.read_time
                    t_m = os.path.getmtime(env.doc2path(docname))
                    if t_m >= t_r:
                        docnames.append(docname)

            # Any others specifically noted for re-reading?
            for path in force_reread_rst_paths or []:
                docname = env.path2doc(path)
                if docname not in docnames:
                    docnames.append(docname)

        def env_updated_handler(app, env):
            """
            This handler is called when Sphinx has finished its READING phase,
            but has not yet begun its RESOLVING phase. We do our own READING
            and RESOLVING, which also in a sense begins the Sphinx RESOLVING
            phase, by resolving all pfsc modules formed from rst files.
            """
            pfsc_env = env.proofscape
            for rst_module in pfsc_env.get_modules().values():
                # .pfsc modules get pickled when constructed by `load_module()`;
                # .rst modules don't have that chance, so it happens here.
                # Note: since we are doing our own pickling of .rst modules, we
                # don't need them to be pickled as a part of the Sphinx environment,
                # for the sake of restoration on subsequent builds; however, we
                # *do* need these modules to stay in the Sphinx environment, because
                # they are still needed during Sphinx's WRITE phase (in particular,
                # for our handler for the Sphinx 'html-page-context' event).
                pickle_module(rst_module)
                # *Could* just pickle, and then `load_module()` would find these
                # modules that way. But we manually store them in the cache now,
                # so that they needn't be read from disk in order to get in there.
                lpv = rst_module.getLibpathV()
                self.module_cache[lpv] = rst_module

            self.read_and_resolve()

        buildername = 'dummy' if just_read_no_write else 'html'
        try:
            with patch_docutils(confdir), docutils_namespace():
                app = Sphinx(sourcedir, confdir, outputdir, doctreedir,
                             buildername, confoverrides=confoverrides)
                app.connect('env-before-read-docs', add_rereads)
                app.connect('env-updated', env_updated_handler)
                app.build(force_all=force_all, filenames=filenames)
        except (SphinxError, Exception) as e:
            traceback.print_exc()
            raise PfscExcep(f'Sphinx error: {e}', PECode.SPHINX_ERROR) from e

    def inject_origins(self):
        visitor = OriginInjectionVisitor(self.mii.origins)
        for module in self.modules.values():
            module.recursiveItemVisit(visitor)

    def check_root_declarations(self):
        """
        This is where we load and perform checks on any of the things that are
        to be declared in repo root modules, like the change log and the
        dependencies.
        """
        is_release = self.is_release_build()
        is_major_inc = self.mii.is_major_version_increment()
        is_major_zero = self.mii.is_major_zero()
        repopath = self.repo_info.libpath
        pi = PathInfo(repopath)
        module_has_contents = pi.get_src_fs_path() is not None
        if not module_has_contents:
            # If it's a release build for a major version increment, there must be a
            # repo root module (so that it can declare a change log).
            if is_release and is_major_inc:
                self.raise_missing_change_log_excep()
            else:
                return
        module = load_module(
            self.repo_info.libpath, version=self.version,
            fail_gracefully=False, caching=CachePolicy.TIME, cache=self.module_cache
        )
        # No need to call module.resolve(), since we are only interested in
        # a couple of assignments made in this module (which need no resolution).
        # Change log
        cl = module.getAsgnValue(pfsc.constants.CHANGE_LOG_LHS)
        if is_release and not is_major_zero:
            if is_major_inc:
                # In this case a change log is required.
                if cl is None:
                    self.raise_missing_change_log_excep()
            else:
                # In this case print a warning if a change log _is_ defined.
                if cl is not None:
                    msg = f'Repo `{repopath}` defines a change log for release `{self.version}`'
                    msg += ', but this is not a major version increment.'
                    # This used to be an exception, as follows:
                    #   raise PfscExcep(msg, PECode.DISALLOWED_REPO_CHANGE_LOG)
                    # but for now we are just printing a warning. We'll see how it goes.
                    print(f'WARNING: {msg}')
        self.mii.set_change_log(cl or {})
        # Dependencies
        deps = module.getAsgnValue(pfsc.constants.DEPENDENCIES_LHS, default={})
        self.repo_dependencies = deps
        if is_release:
            if pfsc.constants.WIP_TAG in deps.values():
                msg = f'Repo `{repopath}` imports from one or more other repos at WIP,'
                msg += ' but this is not allowed in a release build.'
                raise PfscExcep(msg, PECode.NO_WIP_IMPORTS_IN_NUMBERED_RELEASES)

    def walk(self, root_fs_path):
        """
        When building recursively, this method manages the walking of the filesystem hierarchy.
        :param root_fs_path: The filesystem path to the directory we want to walk.
        :return: list of "jobs," being pairs (modpath, tree_node), to be passed to
            our `read_pfsc_module` method.
        """
        jobs = []
        walk_list = list(os.walk(root_fs_path))
        self.monitor.begin_phase(len(walk_list), 'Scanning...')
        for P, D, F in walk_list:
            self.monitor.inc_count()

            # FIXME: really should check first whether we're under a skip dir, and skip immediately.
            #   Wasting lots of cycles walking all through .git!
            #   Might be worth just implementing our own custom "walk" iterator.
            # For now, just a simple check that the path does not contain a hidden dir or file:
            if P.find(os.sep+'.') >= 0: continue

            # What is the ID of the manifest tree node representing the directory we are now in?
            internal_fs_path = os.path.relpath(P, root_fs_path)
            if internal_fs_path == '.':
                parent_node_id = self.repopath
            else:
                parent_node_id = self.repopath + '.' + internal_fs_path.replace(os.sep, '.')
            # Check for a parent node with this ID.
            parent_node = self.manifest.get(parent_node_id)
            # If None, this is because we are in or under a directory that was marked as to be skipped.
            if parent_node is None: continue

            # We'll build a list of child nodes to be added to the parent node.
            child_nodes = []

            # Check for lists of items to be skipped.
            skip_dirs = []
            skip_files = []
            if internal_fs_path in self.skip_items:
                skip_info = self.skip_items[internal_fs_path]
                skip_dirs = skip_info.get('dirs', [])
                skip_files = skip_info.get('files', [])

            # Scan the nested directories.
            for d in D:
                # Skip?
                if d[0] == '.' or d in skip_dirs: continue
                # If not skipping, we add a node to the tree for this directory.
                dir_id = parent_node_id + '.' + d
                # TODO: allow user to define name of directory.
                #   This could be done by defining the name within a special file in the directory.
                #   Maybe just define a "dirname" string in the __.pfsc file in the dir?
                dir_node = ManifestTreeNode(dir_id, type="MODULE", name=d)
                # Record this directory's tree node as one of the parent's children
                child_nodes.append(dir_node)
                self.mii.add_submodule(dir_id, parent_node_id)

            # Now scan the files under this path.
            # FIXME: it would be cool if we could do local depths for _all_ deducs defined in a whole directory.
            #  Right now we only do this within each module. The result is that if you define expansions in a
            #  separate module from literature deducs, say, then the expans are all shown at level 0, instead of
            #  being nicely nested under the deducs they target, as we'd like.
            for f in F:
                # Skip?
                if f[0] == '.' or f in skip_files: continue
                # Is it a module?
                name, ext, other_ext = parse_module_filename(f)
                if name:
                    if (pathlib.Path(P) / f'{name}{other_ext}').exists():
                        msg = (
                            f'Module name `{name}` occurs with both .pfsc and'
                            f' .rst extension in dir `{P}`.'
                        )
                        raise PfscExcep(msg, PECode.MODULE_NAME_USED_WITH_MULTIPLE_EXTENSIONS)
                    # Reconstruct the module's abs libpath.
                    modpath = parent_node_id if name == "__" else parent_node_id + '.' + name
                    if name == "__":
                        # Contents of "dunder module" will be added directly to the parent node.
                        mod_node = parent_node
                        mod_node.set_data_property('hasContents', True)
                    else:
                        # For "terminal modules", add a manifest node to represent the module itself.
                        mod_node = ManifestTreeNode(modpath, type="MODULE", name=name, is_rst=(ext == RST_EXT))
                        child_nodes.append(mod_node)
                        self.mii.add_submodule(modpath, parent_node_id)
                    # Record the job.
                    jobs.append((modpath, mod_node))

            # Sort child nodes, then add to parent node.
            child_nodes.sort(key=lambda n: pfsc.util.NumberedName(n.data.get('name', '')))
            for n in child_nodes:
                parent_node.add_child(n)

        return jobs

    def read_pfsc_module(self, modpath):
        """
        Read a proofscape module, and record it as a scan job.

        :param modpath: the libpath of the module to be processed
        """
        self.monitor.set_message('Reading %s...' % modpath)
        if self.verbose: print("  ", modpath)

        try:
            module = load_module(
                modpath, version=self.version,
                fail_gracefully=False, caching=CachePolicy.TIME, cache=self.module_cache
            )
        except PfscExcep as e:
            if e.code() == PECode.PARSING_ERROR:
                msg = 'Error while parsing module `%s`.' % modpath
                msg += '\n\nDetails:\n\n%s' % e.public_msg()
                raise PfscExcep(msg, PECode.PARSING_ERROR)
            else:
                e.msg = f'While loading module `{modpath}`:\n\n' + e.msg
                raise e

        self.monitor.inc_count(self.prog_count_for_parsing)
        return module

    def scan_pfsc_module(self, module, manifest_node):
        """
        Process a single proofscape module. This means recording dashgraphs and annotations,
        adding manifest tree nodes, and recording indexing info, for the contents of this module.

        :param module: the PfscModule to be scanned
        :param manifest_node: a ManifestTreeNode representing this module, and to which
                              nodes representing its items are to be added
        """
        modpath = module.libpath
        self.monitor.set_message('Scanning %s...' % modpath)

        manifest_node.update_data({'isTerminal': module.isTerminal()})

        # Grab all the items.
        all_items = module.getNativeItemsInDefOrder(hoist_expansions=True)
        num_items = len(all_items)
        remaining_count = self.prog_count_per_module - self.prog_count_for_parsing
        prog_count_per_item = 0
        if num_items == 0:
            self.monitor.inc_count(remaining_count)
        else:
            prog_count_per_item = remaining_count / num_items

        annos = []
        sphinx_page = None
        defns = {}
        asgns = {}
        for name, item in all_items.items():
            if isinstance(item, Annotation):
                annos.append(item)
            elif isinstance(item, SphinxPage):
                # A module can define at most one SphinxPage.
                sphinx_page = item
            elif isinstance(item, PfscDefn):
                defns[name] = item
            elif isinstance(item, Deduction):
                pass
            elif isinstance(item, PfscAssignment):
                asgns[name] = item

        # For the list of all deductions defined within the module, we do request toposort, to help us
        # list the deducs in a nice order for the tree view.
        deducs = module.getAllNativeDeductions(toposort=True, numberednames=True)

        # Lookup for ManifestTreeNodes we will build
        mtns_by_name = {}

        for anno in annos:
            # Record for indexing
            self.mii.add_anno(module, anno)
            # Record the Annotation itself.
            annopath = anno.getLibpath()
            self.annotations[annopath] = anno
            # Doc refs
            # Note: It's not enough to call `anno.gather_doc_info()` here, since
            # it's not until `enrich_data()` has been called on each widget that
            # doc infos will be available. But `anno.get_page_data()` is caching,
            # so this is not wasted effort.
            doc_info = anno.get_page_data()['docInfo']
            self.manifest.update_doc_info(doc_info['docs'])
            # Add a tree node.
            name = anno.getName()
            mtns_by_name[name] = ManifestTreeNode(
                annopath, type="NOTES", name=name,
                modpath=modpath, sourceRow=anno.getFirstRowNum(),
                docRefs=doc_info['refs']
            )
            self.monitor.inc_count(prog_count_per_item)

        if sphinx_page:
            page = sphinx_page
            self.mii.add_sphinx_page(module, page)
            pagepath = page.getLibpath()
            self.sphinx_pages[pagepath] = page

            doc_info = page.get_page_data()['docInfo']
            self.manifest.update_doc_info(doc_info['docs'])

            name = page.getName()
            mtns_by_name[name] = ManifestTreeNode(
                pagepath, type="SPHINX", name=name,
                modpath=modpath, sourceRow=1,
                docRefs=doc_info['refs']
            )
            self.monitor.inc_count(prog_count_per_item)

        for name in defns:
            libpath = f'{module.libpath}.{name}'
            self.mii.add_generic(IndexType.DEFN, libpath, module)
            self.monitor.inc_count(prog_count_per_item)

        for name in asgns:
            libpath = f'{module.libpath}.{name}'
            self.mii.add_generic(IndexType.ASGN, libpath, module)
            self.monitor.inc_count(prog_count_per_item)

        # For each deduc in this module, we will map its libpath to its "depth within the module".
        # Depth within the module is defined as follows:
        #   Any TLD has depth 0, and so does any deduc whose target deduc lies outside this module.
        #   Any deduc E whose target deduc D is defined within this module satisfies depth(E) = depth(D) + 1.
        deduc_depth_within_module = {}

        # Iterate over the deductions defined in this module, in topo-order.
        for deduc in deducs:
            dlp = deduc.getLibpath()
            self.deductions[dlp] = deduc
            # Record deduc for indexing.
            self.mii.add_deduc(module, deduc)
            # Compute and record depth within module.
            # First get the libpath of the "target deduction", i.e. the deduc to which all the target nodes belong.
            tdlp = deduc.getTargetDeducLibpath()
            # (Because we process deducs in topo-order, it follows that tdlp is defined within this module
            #  and is not None IFF it is already a key in the depth lookup.)
            depth = 1 + deduc_depth_within_module.get(tdlp, -1)
            deduc_depth_within_module[dlp] = depth
            # Doc refs
            doc_info = deduc.gather_doc_info()
            self.manifest.update_doc_info(doc_info['docs'])
            # Add a tree node.
            name = deduc.getName()
            mtns_by_name[name] = ManifestTreeNode(
                dlp, type="CHART", name=name, modpath=modpath,
                sourceRow=deduc.getFirstRowNum(), docRefs=doc_info['refs'],
                # target deduc libpath
                tdlp=tdlp,
                # depth
                depth=depth
            )
            self.monitor.inc_count(prog_count_per_item)

        # Add the manifest tree nodes in definition order.
        for name, item in all_items.items():
            mtn = mtns_by_name.get(name)
            if mtn:
                manifest_node.add_child(mtn)

    def write_all(self):
        n = len(self.modules) + len(self.deductions) + len(self.annotations)
        self.monitor.begin_phase(n, 'Writing...')
        self.clear_build_dirs()
        if self.build_in_gdb:
            self.copy_src_into_gdb()
        self.write_manifest()
        self.write_dashgraphs()
        self.write_notespages()

    def write_manifest(self):
        d = self.manifest.build_dict()
        j = json.dumps(d, indent=4)
        if self.build_in_gdb:
            self.graph_writer.record_repo_manifest(self.repo_info.libpath, self.version, j)
        else:
            build_dir = self.repo_info.get_build_dir(version=self.version)
            manifest_json_path = self.repo_info.get_manifest_json_path(version=self.version)
            os.makedirs(build_dir, exist_ok=True)
            with open(manifest_json_path, 'w') as f:
                f.write(j)

    def copy_src_into_gdb(self):
        """
        If storing builds in GDB, we copy of module source code in there too.
        """
        if self.build_in_gdb:
            for module in self.modules.values():
                text = module.getBuiltVersion()
                modpath = module.getLibpath()
                self.graph_writer.record_module_source(modpath, self.version, text)
                self.monitor.inc_count()

    def clear_build_dirs(self):
        """
        Clean out the build directory for each built module. This eliminates old built
        products for any entities that used to be defined in these modules, but no
        longer are.
        """
        for module in self.modules.values():
            if self.build_in_gdb:
                modpath = module.getLibpath()
                self.graph_writer.delete_builds_under_module(modpath, self.version)
            else:
                src_path = module.get_build_dir_src_code_path(version=self.version)
                build_dir = src_path.parent
                if build_dir.exists():
                    for path in build_dir.iterdir():
                        if path.is_file() and path.suffixes in [
                            ['.anno', '.html'],
                            ['.anno', '.json'],
                            ['.dg', '.json'],
                        ]:
                            path.unlink()

    def write_dashgraphs(self):
        """
        Write the dashgraphs to disk.
        """
        for deducpath, deduc in self.deductions.items():
            dashgraph = deduc.buildDashgraph()
            dg_json = json.dumps(dashgraph, indent=4)
            if self.build_in_gdb:
                self.graph_writer.record_dashgraph(deducpath, self.version, dg_json)
            else:
                dest_dir, filename = get_dashgraph_dir_and_filename(deducpath, version=self.version)
                os.makedirs(dest_dir, exist_ok=True)
                dg_json_path = os.path.join(dest_dir, filename)
                with open(dg_json_path, 'w') as f:
                    f.write(dg_json)
            self.monitor.inc_count()

    def write_notespages(self):
        """
        Write the annotations to disk.
        """
        for annopath, annotation in self.annotations.items():
            anno_html = annotation.get_escaped_html()
            anno_json = json.dumps(annotation.get_page_data(), indent=4)
            if self.build_in_gdb:
                self.graph_writer.record_annobuild(annopath, self.version, anno_html, anno_json)
            else:
                dest_dir, html_filename, json_filename = get_annotation_dir_and_filenames(annopath, version=self.version)
                os.makedirs(dest_dir, exist_ok=True)
                anno_html_path = os.path.join(dest_dir, html_filename)
                anno_json_path = os.path.join(dest_dir, json_filename)
                with open(anno_html_path, 'w') as f:
                    f.write(anno_html)
                with open(anno_json_path, 'w') as f:
                    f.write(anno_json)
            self.monitor.inc_count()

    def update_index(self):
        """
        Update the graph database.
        """
        self.mii.setup_monitor()
        self.graph_writer.index_module(self.mii)


def index(obj):
    """
    Convenience function to accept a variety of arguments, and perform just the indexing operation
    on the appropriate module.

    :param obj: Can be a variety of argument types:

                libpath (str): we construct a Builder on this libpath, and ask it to index.
                RepoInfo: we construct a Builder on this RepoInfo's libpath, and ask it to index.
                Builder: we ask it to index.

    :return: The report from the indexing operation.
    """
    if isinstance(obj, str):
        b = Builder(obj)
    elif isinstance(obj, RepoInfo):
        b = Builder(obj.libpath)
    elif isinstance(obj, Builder):
        b = obj
    else:
        raise PfscExcep("Unrecognized type.")
    # Build.
    b.build()
    # And index.
    return b.update_index()
